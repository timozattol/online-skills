{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from crawler import async_cache_pages, urls_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# edX does not work as some content seems to be dynamically downloaded\n",
    "folders = [\"coursera\", \"general\"]\n",
    "\n",
    "positives, negatives = urls_list(folders)\n",
    "async_cache_pages(positives + negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_functions = {\n",
    "        #\"url\": url,\n",
    "        #\"label\": label,\n",
    "        \"a_count\": lambda soup: len(soup.find_all(\"a\")),\n",
    "        \"iframe_count\": lambda soup: len(soup.find_all(\"iframe\")),\n",
    "        \"h1_count\": lambda soup: len(soup.find_all(\"h1\")),\n",
    "        \"h2_count\": lambda soup: len(soup.find_all(\"h2\")),\n",
    "        \"h3_count\": lambda soup: len(soup.find_all(\"h3\")),\n",
    "        \"video_count\": lambda soup: len(soup.find_all(\"video\")),\n",
    "        \"button_count\": lambda soup: len(soup.find_all(\"button\"))\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from crawler import get_cached, is_cached\n",
    "\n",
    "def construct_df_features(urls, labels, features):\n",
    "    row_list = []\n",
    "    features_func = [feature_functions[feat] for feat in features]\n",
    "    columns = [\"url\", \"label\"] + features\n",
    "\n",
    "    for url, label in zip(urls, labels):\n",
    "        if is_cached(url):\n",
    "            soup = BeautifulSoup(get_cached(url), \"lxml\")\n",
    "\n",
    "        row = [url, label] + [func(soup) for func in features_func]\n",
    "\n",
    "        row_list.append(row)\n",
    "\n",
    "    df = pd.DataFrame(row_list, columns=columns)\n",
    "    df = df.set_index(\"url\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls = [positives[0], negatives[0]]\n",
    "labels = [True, False]\n",
    "features = [\"button_count\", \"a_count\"]\n",
    "\n",
    "construct_df_features(urls, labels, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_features = sorted(list(feature_functions.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_val(df, selected_features, estimator=RandomForestClassifier(), cv=3):\n",
    "    X = df[selected_features]\n",
    "    y = df[\"label\"]\n",
    "    \n",
    "    return cross_val_score(estimator, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls_pos, urls_neg = urls_list([\"coursera\", \"general\"])\n",
    "urls = urls_pos + urls_neg\n",
    "labels = [True] * len(urls_pos) + [False] * len(urls_neg)\n",
    "\n",
    "results_means = []\n",
    "results_stds = []\n",
    "\n",
    "df = construct_df_features(urls, labels, all_features)\n",
    "\n",
    "# iterate over all features and train on this feature alone\n",
    "for feature in all_features:\n",
    "    # Cross val with the selected feature alone\n",
    "    scores = cross_val(df, [feature])\n",
    "    \n",
    "    results_means.append(np.mean(scores))\n",
    "    results_stds.append(np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Same thing, but with same ratio of positive and negative examples\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "urls_pos, urls_neg = urls_list([\"coursera\", \"general\"])\n",
    "\n",
    "# Same number of negative and positive examples\n",
    "urls = urls_pos + random.sample(urls_neg, len(urls_pos))\n",
    "labels = [True] * len(urls_pos) + [False] * len(urls_pos)\n",
    "\n",
    "results_unbiased_means = []\n",
    "results_unbiased_stds = []\n",
    "\n",
    "df_unbiased = construct_df_features(urls, labels, all_features)\n",
    "\n",
    "# iterate over all features and train on one feature alone\n",
    "for feature in all_features:\n",
    "    # Cross val with the selected feature alone\n",
    "    scores = cross_val(df_unbiased, [feature])\n",
    "    \n",
    "    results_unbiased_means.append(np.mean(scores))\n",
    "    results_unbiased_stds.append(np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "index = np.arange(len(results_means))\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "bar_width = 0.35\n",
    "axis_font_size = 18\n",
    "\n",
    "plt.bar(index, results_means, bar_width, yerr=results_stds, color='tab:red', label=\"Biased estimator\")\n",
    "plt.bar(index + bar_width, results_unbiased_means, bar_width, yerr=results_unbiased_stds, label=\"Unbiased estimator\")\n",
    "\n",
    "plt.xlabel(\"Feature\", fontsize=axis_font_size)\n",
    "plt.ylabel(\"Cross validation score\", fontsize=axis_font_size)\n",
    "plt.xticks(index + bar_width / 2, all_features, fontsize=14)\n",
    "\n",
    "plt.axhline(y=0.5, color='tab:brown', alpha=0.7, linewidth=1)\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "plt.savefig(os.path.join(\"figures\", \"feature_score_biased_unbiased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "X = df_unbiased.drop(\"label\", axis=1)\n",
    "y = df_unbiased[\"label\"]\n",
    "\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_importances = rf.feature_importances_\n",
    "feature_importances_stds = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "index = np.arange(len(results_means))\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "bar_width = 0.35\n",
    "axis_font_size = 18\n",
    "\n",
    "plt.bar(index, results_unbiased_means, bar_width, yerr=results_unbiased_stds, label=\"Unbiased estimator\")\n",
    "plt.bar(index + bar_width, feature_importances, bar_width, color='tab:gray', yerr=feature_importances_stds, label=\"Feature importances\")\n",
    "\n",
    "plt.xlabel(\"Feature\", fontsize=axis_font_size)\n",
    "plt.ylabel(\"Cross validation score / Feature importance\", fontsize=axis_font_size)\n",
    "plt.xticks(index + bar_width / 2, all_features, fontsize=14)\n",
    "\n",
    "plt.axhline(y=0.5, color='tab:brown', alpha=0.7, linewidth=1)\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "plt.savefig(os.path.join(\"figures\", \"feature_importance_and_feature_score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_score(df, selected_features, estimator=RandomForestClassifier()):\n",
    "    X = df[selected_features]\n",
    "    y = df[\"label\"]\n",
    "    \n",
    "    estimator.fit(X, y)\n",
    "    y_pred = estimator.predict(X)\n",
    "    \n",
    "    return accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_features = [\"a_count\", \"button_count\", \"h1_count\", \"h2_count\", \"h3_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_estims_list = range(1, 21)\n",
    "score_means = []\n",
    "score_stds = []\n",
    "train_scores = []\n",
    "features = good_features\n",
    "\n",
    "for n_estims in n_estims_list:\n",
    "    scores = cross_val(df_unbiased, features, estimator=RandomForestClassifier(n_estimators=n_estims))\n",
    "    \n",
    "    score_means.append(np.mean(scores))\n",
    "    score_stds.append(np.std(scores))\n",
    "    \n",
    "    train_score = training_score(df_unbiased, features, estimator=RandomForestClassifier(n_estimators=n_estims))\n",
    "    train_scores.append(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# No apparent overfitting (?)\n",
    "plt.plot(n_estims_list, score_means)\n",
    "plt.plot(n_estims_list, train_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is not supposed to work well, because \". SVMs do not perform well on highly skewed/imbalanced data sets. These are training data sets in which the number of samples that fall in one of the classes far outnumber those that are a member of the other class. Customer churn data sets are typically in this group because when you collect the training set, among a million customers during a particular time period, there would be very few who have actually churned. SMOTING is used to generate artificial samples in the minority class to balance the data set. On the other hand, Logistic Regression is good at handling skewed data sets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "kernels = ['linear', 'sigmoid', 'rbf']\n",
    "score_means = []\n",
    "score_stds = []\n",
    "train_scores = []\n",
    "features = good_features\n",
    "\n",
    "for kernel in kernels:\n",
    "    scores = cross_val(df_unbiased, features, estimator=svm.SVC(kernel=kernel))\n",
    "    \n",
    "    score_means.append(np.mean(scores))\n",
    "    score_stds.append(np.std(scores))\n",
    "    \n",
    "    train_score = training_score(df_unbiased, features, estimator=svm.SVC(kernel=kernel))\n",
    "    train_scores.append(train_score)\n",
    "    \n",
    "plt.plot(score_means)\n",
    "plt.plot(train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n_neigbours = range(1, 40)\n",
    "score_means = []\n",
    "score_stds = []\n",
    "train_scores = []\n",
    "features = good_features\n",
    "\n",
    "for n in n_neigbours:\n",
    "    scores = cross_val(df_unbiased, features, estimator=KNeighborsClassifier(n_neighbors=n))\n",
    "    \n",
    "    score_means.append(np.mean(scores))\n",
    "    score_stds.append(np.std(scores))\n",
    "    \n",
    "    train_score = training_score(df_unbiased, features, estimator=KNeighborsClassifier(n_neighbors=n))\n",
    "    train_scores.append(train_score)\n",
    "    \n",
    "plt.plot(n_neigbours, score_means)\n",
    "plt.plot(n_neigbours, train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "layers = range(1, 10)\n",
    "score_means = []\n",
    "score_stds = []\n",
    "train_scores = []\n",
    "features = good_features\n",
    "\n",
    "for layer in layers:\n",
    "    scores = cross_val(df_unbiased, features, estimator=MLPClassifier(hidden_layer_sizes=(30,)*layer, random_state=1))\n",
    "    \n",
    "    score_means.append(np.mean(scores))\n",
    "    score_stds.append(np.std(scores))\n",
    "    \n",
    "    train_score = training_score(df_unbiased, features, estimator=MLPClassifier(hidden_layer_sizes=(30,)*layer, random_state=1))\n",
    "    train_scores.append(train_score)\n",
    "    \n",
    "plt.plot(layers, score_means)\n",
    "plt.plot(layers, train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "reg_terms = np.arange(0.1, 1, 0.05)\n",
    "score_means = []\n",
    "score_stds = []\n",
    "train_scores = []\n",
    "features = good_features\n",
    "\n",
    "for reg in reg_terms:\n",
    "    scores = cross_val(df_unbiased, features, estimator=LogisticRegression(C=reg, random_state=1))\n",
    "    \n",
    "    score_means.append(np.mean(scores))\n",
    "    score_stds.append(np.std(scores))\n",
    "    \n",
    "    train_score = training_score(df_unbiased, features, estimator=LogisticRegression(C=reg, random_state=1))\n",
    "    train_scores.append(train_score)\n",
    "    \n",
    "plt.plot(reg_terms, score_means)\n",
    "plt.plot(reg_terms, train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
